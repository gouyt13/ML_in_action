{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd082b6f5812ea506d741ac7a355c38975a8264e75987c5404f7effc78bc7e6db28",
   "display_name": "Python 3.8.10 64-bit ('d2l': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"词表到向量的转换函数\"\"\"\n",
    "    posting_list=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    class_vec = [0,1,0,1,0,1]    # 1代表侮辱性文字，0代表正常言论\n",
    "    return posting_list, class_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_list(dataset):\n",
    "    \"\"\"创建单词去重列表\"\"\"\n",
    "    vocab_set = set()\n",
    "    for document in dataset:\n",
    "        vocab_set = vocab_set | set(document)\n",
    "    return list(vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_of_words_vec(vocab_list, input_set):\n",
    "    \"\"\"获得单词向量\"\"\"\n",
    "    return_vec = [0] * len(vocab_list)\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            return_vec[vocab_list.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word %s is not in my Vocabulary\" % word)\n",
    "    return return_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['park', 'love', 'is', 'dog', 'worthless', 'ate', 'please', 'so', 'stupid', 'posting', 'mr', 'maybe', 'my', 'buying', 'has', 'dalmation', 'help', 'I', 'stop', 'cute', 'how', 'quit', 'food', 'garbage', 'flea', 'take', 'not', 'steak', 'to', 'him', 'licks', 'problems']\n"
     ]
    }
   ],
   "source": [
    "list_of_posts, list_classes = load_dataset()\n",
    "my_vocablist = create_vocab_list(list_of_posts)\n",
    "print(my_vocablist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n[0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(set_of_words_vec(my_vocablist, list_of_posts[0]))\n",
    "print(set_of_words_vec(my_vocablist, list_of_posts[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def train_NB0(train_matrix, train_category):\n",
    "    \"\"\"朴素贝叶斯分类器训练函数\"\"\"\n",
    "    num_train_docs = len(train_matrix)\n",
    "    num_words = len(train_matrix[0])    # 词汇表所含单词数\n",
    "    p_abusive = sum(train_category) / float(num_train_docs) # 是侮辱性文档的概率\n",
    "    p0_num = np.ones(num_words)\n",
    "    p1_num = np.ones(num_words) # 这里不初始化np.zero()是考虑在朴素贝叶斯中计算乘积一个为0导致结果为0\n",
    "    p0_denom = 2.0  # 类似，这里不初始化为0\n",
    "    p1_denom = 2.0  # 初始化概率\n",
    "\n",
    "    for i in range(num_train_docs):\n",
    "        if train_category[i] == 1:\n",
    "            p1_num += train_matrix[i]\n",
    "            p1_denom += sum(train_matrix[i])    # 向量相加\n",
    "        else:\n",
    "            p0_num += train_matrix[i]\n",
    "            p0_denom += sum(train_matrix[i])    # 对于两类文档，分别加上某个文档词汇表中出现的词的计数\n",
    "    p1_vect = np.log(p1_num / p1_denom)     # 使用log防止下溢\n",
    "    p0_vect = np.log(p0_num / p0_denom)\n",
    "    return p0_vect, p1_vect, p_abusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_posts, list_classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocablist = create_vocab_list(list_of_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = [] \n",
    "for postin_doc in list_of_posts:\n",
    "    train_mat.append(set_of_words_vec(my_vocablist, postin_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0], [0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0], [0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0_v, p1_v, p_ab = train_NB0(train_mat, list_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "p_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-3.25809654, -2.56494936, -2.56494936, -2.56494936, -3.25809654,\n",
       "       -2.56494936, -2.56494936, -2.56494936, -3.25809654, -3.25809654,\n",
       "       -2.56494936, -3.25809654, -1.87180218, -3.25809654, -2.56494936,\n",
       "       -2.56494936, -2.56494936, -2.56494936, -2.56494936, -2.56494936,\n",
       "       -2.56494936, -3.25809654, -3.25809654, -3.25809654, -2.56494936,\n",
       "       -3.25809654, -3.25809654, -2.56494936, -2.56494936, -2.15948425,\n",
       "       -2.56494936, -2.56494936])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "p0_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-2.35137526, -3.04452244, -3.04452244, -1.94591015, -1.94591015,\n",
       "       -3.04452244, -3.04452244, -3.04452244, -1.65822808, -2.35137526,\n",
       "       -3.04452244, -2.35137526, -3.04452244, -2.35137526, -3.04452244,\n",
       "       -3.04452244, -3.04452244, -3.04452244, -2.35137526, -3.04452244,\n",
       "       -3.04452244, -2.35137526, -2.35137526, -2.35137526, -3.04452244,\n",
       "       -2.35137526, -2.35137526, -3.04452244, -2.35137526, -2.35137526,\n",
       "       -3.04452244, -3.04452244])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "p1_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_NB(vec2classify, p0_vec, p1_vec, p_class1):\n",
    "    \"\"\"朴素贝叶斯分类函数\"\"\"\n",
    "    p1 = np.sum(vec2classify * p1_vec) + np.log(p_class1)   # 对应元素相乘\n",
    "    p0 = np.sum(vec2classify * p0_vec) + np.log(1.0 - p_class1)\n",
    "    if p1 > p0:\n",
    "        return 1 \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def testing_NB():\n",
    "    \"\"\"\n",
    "    其实就是把上面的代码搞到了一起\n",
    "    用两个非常简单的例子测试一下\n",
    "    \"\"\"\n",
    "    list_of_posts, list_classes = load_dataset()\n",
    "    my_vocablist = create_vocab_list(list_of_posts)\n",
    "    train_mat = []\n",
    "    for postin_doc in list_of_posts:\n",
    "        train_mat.append(set_of_words_vec(my_vocablist, postin_doc))\n",
    "    \n",
    "    test_entry = ['love', 'my', 'dalmation']\n",
    "    this_doc = np.array(set_of_words_vec(my_vocablist, test_entry))\n",
    "    print(test_entry, 'classified as: ', classify_NB(this_doc, p0_v, p1_v, p_ab))\n",
    "\n",
    "    test_entry = ['stupid', 'garbage']\n",
    "    this_doc = np.array(set_of_words_vec(my_vocablist, test_entry))\n",
    "    print(test_entry, 'classified as: ', classify_NB(this_doc, p0_v, p1_v, p_ab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testing_NB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words2vec_mn(vocab_list, input_set):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯词袋模型\n",
    "    不仅关注某个词是否出现，还关心出现的频率\n",
    "    \"\"\"\n",
    "    return_vec = [0] * len(vocab_list)\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            return_vec[vocab_list.index(word)] += 1\n",
    "    return return_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sent = 'This book is the best book for Python or M.L. I have ever laid eyes upon.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'for',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M.L.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon.']"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "my_sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'for', 'Python', 'or', 'M', 'L', '', 'I', 'have', 'ever', 'laid', 'eyes', 'upon', '']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "reg_ex = re.compile(r'\\W')\n",
    "list_of_tokens = reg_ex.split(my_sent)\n",
    "print(list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'for',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M',\n",
       " 'L',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "[tok for tok in list_of_tokens if tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['this',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'for',\n",
       " 'python',\n",
       " 'or',\n",
       " 'm',\n",
       " 'l',\n",
       " 'i',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "[tok.lower() for tok in list_of_tokens if tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('email/ham/6.txt', encoding='gbk') as f:\n",
    "    email_text = f.read()\n",
    "list_of_tokens = reg_ex.split(email_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the error rate is 0.000000\n"
     ]
    }
   ],
   "source": [
    "def text_parse(big_string):\n",
    "    import re \n",
    "    list_of_tokens = re.split(r'\\W', big_string)\n",
    "    return [tok.lower() for tok in list_of_tokens if tok]\n",
    "\n",
    "def spam_test():\n",
    "    doc_list = []\n",
    "    class_list = []\n",
    "    full_text = []\n",
    "    for i in range(1, 26):\n",
    "        with open('email/spam/%d.txt' % i, encoding='ISO-8859-1') as f1:\n",
    "            word_list = text_parse(f1.read())\n",
    "            doc_list.append(word_list)\n",
    "            full_text.extend(word_list)\n",
    "            class_list.append(1)\n",
    "        with open('email/ham/%d.txt' % i, encoding='ISO-8859-1') as f2:\n",
    "            word_list = text_parse(f2.read())\n",
    "            doc_list.append(word_list)\n",
    "            full_text.extend(word_list)\n",
    "            class_list.append(0)\n",
    "    \n",
    "    vocab_list = create_vocab_list(doc_list)\n",
    "    test_set = list(np.random.choice(50,10,replace=False))  # 从50个中不重复随机选择10个\n",
    "    training_set = [i for i in range(50) if i not in test_set]  # 剩下的作为训练集\n",
    "\n",
    "    train_mat = []; train_class = []\n",
    "    for doc_index in training_set:\n",
    "        train_mat.append(set_of_words_vec(vocab_list, doc_list[doc_index]))\n",
    "        train_class.append(class_list[doc_index])\n",
    "    p0_v, p1_v, p_spam = train_NB0(np.array(train_mat), np.array(train_class))\n",
    "    error_count = 0\n",
    "    for doc_index in test_set:\n",
    "        word_vector = set_of_words_vec(vocab_list, doc_list[doc_index])\n",
    "        if classify_NB(np.array(word_vector), p0_v, p1_v, p_spam) !=\\\n",
    "            class_list[doc_index]:\n",
    "            error_count += 1\n",
    "    print(\"the error rate is %f\" % (float(error_count) / len(test_set))) \n",
    "\n",
    "spam_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}